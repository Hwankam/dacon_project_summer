---
layout : post
title : Dacon - 구내식당 식수 인원 예측 AI 경진대회
subtitle : 1. 대회를 끝내며 남기는 소감
date : 2021-07-28
#categories:
tags: [datascience, dacon, machine learning]
toc_sticky : True
comments: true
---

---

처음으로 참여했던 데이콘 대회에서 프로젝트 과정 중에 느끼고 배웠던 내용들을 정리해보고자 합니다. 

이 글은 그에 대한 첫 번째 글로써, 프로젝트에 대한 개괄적 설명과 프로젝트를 진행하면서 고민했던 점을 글로 남겨보도록 하겠습니다. 파이썬을 사용해서 프로젝트를 진행했기 때문에 다소 미흡한 부분이 있습니다. 앞으로 파이썬 실력을 키우기 위해 더욱 노력해야 하겠습니다.

public 순위 : 162등,
private 순위 : 81등



### 1. 대회 데이터 설명

이 대회는 한국토지주택공사(이하 LH)가 주최한 대회로, 중식과  석식 식수인원을 예측하는 데에 목적이 있습니다.  train data set와 test data set의 column은 다음과 같습니다.

```python
pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)
train = pd.read_csv("/dacon/주차/train.csv")
test = pd.read_csv("/dacon/주차/test.csv")

train.columns
#Index(['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수','현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴', '중식계', '석식계'],dtype='object')

test.columns
#Index(['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수','현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴'],dtype='object')

train.shape
#(1205,12)
test.shpae
#(50,10)
```



특이한 점은 데이터의 절대적인 개수가 다른 대회에 비해 매우 적은 편이며, column 또한 매우 적습니다. 즉,  1200개의 데이터를 가지고 의미있는 모델을 만들기 위해서는 변수의 개수를 잘 통제해서 '차원의 저주'에 빠지지 않도록 해야하며, 딥러닝 모델을 사용하기는 어렵지 않을까 하는 생각을 했습니다.



### 2. 대회 도중의 시행착오와 해결책

#### 1. 시계열 데이터를 다루는 방법

이번 대회는 시계열 데이터를 다루고 있습니다. 통계학적 측면에서 볼 때 시계열 자료의 특성상 자기 상관성을 가지고 있고 저는 이를 어떻게 처리할 수 있을 지에 대한 고민을 많이 했습니다. K-fold cross validation 같은 경우에도 함부로 데이터를 나누게 되면 미래의 데이터로 과거를 검증하게 되는 문제가 발생합니다. 또한 전통적으로 통계학적 관점에서 시계열 데이터를 다루는 방법인 ARIMA같은 경우에는 오로지 시간만을 가지고 분석을 하기 때문에 그 자체로 한계를 가진다고 생각했습니다. 페이스북에서 만들어낸 prophet 같은 알고리즘 또한 시간을 제외한 다양한 변수를 고려하기에는 한계가 있었고, 신경망 알고리즘에서 시간이라는 변수를 추가할 수 있는 RNN이나 LSTM 같은 딥러닝 알고리즘 또한 대회에서 제공하는 데이터의 row가 너무 작기 때문에 한계가 있었습니다. 

결과적으로 시간을 나타내는 데이터를 년,월,일,요일로 모두 쪼개어서 데이터를 새롭게 인코딩했고 꽤나 괜찮은 성능을 만들어내었습니다. 이는 제가 생각한 방법이 아니라 캐글이나 데이콘에서 ranking이 높은 사람들 또한 시계열 데이터를 위와 같이 접근하는 것을 보고 따라서 적용해 본 것입니다. 그럼에도 불구하고 이러한 방식을 통해 데이터가 가지고 있는 계절성과 자기상관성을 충분히 모델에 반영한 것인지는 의문이 듭니다. 또한 년/월/일/주/요일을 모두 one-hot  encoding으로 변환하면 차원이 매우 커진다는 문제가 발생해서 일부의 columns은 사용하지 않았습니다. 이러한 부분은 ''시간''에 대한 충분한 고려가 되었는지 의문이 들게 하는 부분입니다. 그럼에도 불구하고  기존 이론에 얽매이지 않고 한정된 데이터에서 어떻게 하면 좋은 결과를 찾아낼 수 있을지, 기존 상황을 극복하기 위해서 어떤 방법으로 문제를 해결해 나가야 할 지에 대해 생각을 많이 해본 경험이었습니다.



#### 2 . feature engineering의 중요성

뛰어난 성능을 가지기 위해서는 어떠한 모델을 사용할 것인가 보다 핵심 변수/ 영향력 있는 변수가 무엇인지 찾아내는 것이 훨씬 중요하다는 것을 깨달았습니다. 흔히 우리는 머신러닝에서 최대한 많은 정보를 집어 넣으면 기계가 알아서 유의미한 정보를 뽑아내줄 것이라는 착각에 빠지게 됩니다. 저 또한 통계학을 배우면서 ‘차원의 저주’라는 말을 수없이 들었음에도 처음 모델을 짤 때에는 적당히 유의미하다고 여겨지는 변수를 모두 넣어보려는 시도를 했습니다. 만약 얻을 수 있는 데이터의 개수가 매우 많다면 상관없겠지만, 현실에서 이는 사실상 불가능 합니다. 따라서 한정된 데이터를 가지고 분류나 회귀의 모델을 만들 때에는 데이터를 잘 설명해주는 feature를 잘 뽑아내는 것이  중요했습니다.  

저는 데이터 전처리를 끝내자마자 모든 변수를 넣고 최선의 모델이 무엇일지, 최적의 하이퍼파라미터는 무엇일지에 찾는 데에만 많은 시간을 보냈습니다.  xgboost나 LGBM과 같이 자주 들어오던 트리 기반의 모델들이 알아서 데이터 속에 내재된 숨겨진 패턴들을 찾아줄 것이라고 착각했던 것 같습니다. 그러나 차원이 너무 큰 데에 반해 데이터 수는 상대적으로 작아 원하는 성능을 얻기는 어려웠습니다. 이때 저는 외부데이터를 가지고 와서 '확진자수', '현재 기온', '강수량' 같은 영향력 있는 feature를 찾아내었고 보다 성능을 향상시킬 수 있었습니다.  대회 이후 순위권에 올라가신 다른 분들의 코드를 보니 다른 참여자분들은 불쾌지수, 그리고 메뉴데이터 임베딩을 통해 새로운 feature들을 잡아내셨습니다. 

이를 통해 추후 데이터 분석에서는 모델에 집중하기 보다는 데이터 전처리와 feature 추출에 보다 많은 시간을 투자해서 데이터 자체에 대해 깊은 이해를 해 나가야 한다고 생각했습니다. 그리고 또한 다중공선성을 고려할 때 재귀적 방식으로 가장 성능이 높은 변수를 추출해주는 코드 또한 존재한다는 것을 알게되었습니다. 이를 활용한다면 feature engineering을 하는 데 큰 도움이 될 것이라 생각합니다.



#### 3. Domain 지식의 중요성

feature engineering의 중요성에 대한 연장선상에서 domain 지식 또한 중요함을 몸소 체험했습니다. 머신러닝을 위해 데이터를 전처리하고 중요한 특징(feature)를 찾아내야만 알고리즘의 성능이 좋아지는데 이를 위해서는 분석하고자 하는 정보에 대한 지식이 매우 중요함을 깨달았습니다. 이번 대회에서도 예측하고자 하는 식수인원이 야근신청인원과 큰 상관관계를 가지고 있었는데, 이 야근신청인원이 9월에서 11월 사이에 주기적으로 상승하는 부분이 왜 발생하는지, 우연에 의해서 발생한 것인지 궁금했었습니다. 제가 그 회사 직원이 아니기 때문에 이를 명확하게 알기는 어려웠으나 만약 한국주택토지공사 본사 직원이 이에 대해 설명해준다면 이상치 처리나 특징 추출 단계에서 이를 잘 반영할 수 있었을 것입니다. 

저는 통계적 분석 능력을 가지고 머신러닝 구현능력과 알고리즘에 남들보다 특화된 인재가 되고 싶기 때문에 분석 대상에 대한 도메인 지식이 부족할 수 밖에 없습니다. 따라서 머신러닝을 통해 문제를 해결할 때에는 도메인 지식을 가진 분과의 협업이 매우 중요하며,  협업의 과정에서 생기는 마찰들을 잘 조율하는 것 또한 성능을 결정짓는데 중요할 것이라 생각됩니다.



#### 4. data set 자체에 대한 이해 (train / test)

모델을 만들때에는 현재 가지고 있는 데이터를 최적화하는 모델을 만들지만, 실제로 이 모델을 사용하고자 할 때는 완전히 새로운 데이터를 사용하게 됩니다. 이는 모델의 일반성과도 연결되는 부분인데, 테스트 하고자 하는 데이터가 기존의 데이터와 다른 분포를 가지고 있거나, 어떤 외부적인 일로 새로운 경향을 나타내게 될 때는 이를 고려해서 모델을 보다 일반화시키는 것이 중요하다는 생각이 들었습니다. 대회에서 테스트하고 싶은 데이터는 21년 2,3,4월 데이터로 이 당시 LH 땅 투기 사태로 인해 전반적으로 야근인원이 늘었고 식수 인원 또한 직전 분기에 비해 다른 추세를 보일 것이라 생각했었습니다. k-fold 교차 검증에서 상대적으로 모델의 성능이 좋지 않았음에도 불구하고, 모델의 일반성에 초점을 맞춘 결과 테스트 하고자 하는 데이터를 보다 잘 맞출 수 있었습니다. 추후 분석에도 데이터의 분포와 외부적인 상황을 복합적으로 고려할 줄 아는 분석가가 되어야겠다고 생각했습니다.





